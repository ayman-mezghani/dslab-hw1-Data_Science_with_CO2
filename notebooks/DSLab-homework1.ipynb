{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSLab Homework 1 - Data Science with CO2\n",
    "\n",
    "## Hand-in Instructions\n",
    "\n",
    "- __Due: 23.03.2021 23h59 CET__\n",
    "- `git push` your final verion to the master branch of your group's Renku repository before the due\n",
    "- check if `Dockerfile`, `environment.yml` and `requirements.txt` are properly written\n",
    "- add necessary comments and discussion to make your codes readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Carbosense\n",
    "\n",
    "The project Carbosense establishes a uniquely dense CO2 sensor network across Switzerland to provide near-real time information on man-made emissions and CO2 uptake by the biosphere. The main goal of the project is to improve the understanding of the small-scale CO2 fluxes in Switzerland and concurrently to contribute to a better top-down quantification of the Swiss CO2 emissions. The Carbosense network has a spatial focus on the City of Zurich where more than 50 sensors are deployed. Network operations started in July 2017.\n",
    "\n",
    "<img src=\"http://carbosense.wdfiles.com/local--files/main:project/CarboSense_MAP_20191113_LowRes.jpg\" width=\"500\">\n",
    "\n",
    "<img src=\"http://carbosense.wdfiles.com/local--files/main:sensors/LP8_ZLMT_3.JPG\" width=\"156\">  <img src=\"http://carbosense.wdfiles.com/local--files/main:sensors/LP8_sensor_SMALL.jpg\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description of the homework\n",
    "\n",
    "In this homework, we will curate a set of **CO2 measurements**, measured from cheap but inaccurate sensors, that have been deployed in the city of Zurich from the Carbosense project. The goal of the exercise is twofold: \n",
    "\n",
    "1. Learn how to deal with real world sensor timeseries data, and organize them efficiently using python dataframes.\n",
    "\n",
    "2. Apply data science tools to model the measurements, and use the learned model to process them (e.g., detect drifts in the sensor measurements). \n",
    "\n",
    "The sensor network consists of 46 sites, located in different parts of the city. Each site contains three different sensors measuring (a) **CO2 concentration**, (b) **temperature**, and (c) **humidity**. Beside these measurements, we have the following additional information that can be used to process the measurements: \n",
    "\n",
    "1. The **altitude** at which the CO2 sensor is located, and the GPS coordinates (latitude, longitude).\n",
    "\n",
    "2. A clustering of the city of Zurich in 17 different city **zones** and the zone in which the sensor belongs to. Some characteristic zones are industrial area, residential area, forest, glacier, lake, etc.\n",
    "\n",
    "## Prior knowledge\n",
    "\n",
    "The average value of the CO2 in a city is approximately 400 ppm. However, the exact measurement in each site depends on parameters such as the temperature, the humidity, the altitude, and the level of traffic around the site. For example, sensors positioned in high altitude (mountains, forests), are expected to have a much lower and uniform level of CO2 than sensors that are positioned in a business area with much higher traffic activity. Moreover, we know that there is a strong dependence of the CO2 measurements, on temperature and humidity.\n",
    "\n",
    "Given this knowledge, you are asked to define an algorithm that curates the data, by detecting and removing potential drifts. **The algorithm should be based on the fact that sensors in similar conditions are expected to have similar measurements.** \n",
    "\n",
    "## To start with\n",
    "\n",
    "The following csv files in the `../data/carbosense-raw/` folder will be needed: \n",
    "\n",
    "1. `CO2_sensor_measurements.csv`\n",
    "    \n",
    "   __Description__: It containts the CO2 measurements `CO2`, the name of the site `LocationName`, a unique sensor identifier `SensorUnit_ID`, and the time instance in which the measurement was taken `timestamp`.\n",
    "    \n",
    "2. `temperature_humidity.csv`\n",
    "\n",
    "   __Description__: It contains the temperature and the humidity measurements for each sensor identifier, at each timestamp `Timestamp`. For each `SensorUnit_ID`, the temperature and the humidity can be found in the corresponding columns of the dataframe `{SensorUnit_ID}.temperature`, `{SensorUnit_ID}.humidity`.\n",
    "    \n",
    "3. `sensor_metadata.csv`\n",
    "\n",
    "   __Description__: It contains the name of the site `LocationName`, the zone index `zone`, the altitude in meters `altitude`, the longitude `lon`, and the latitude `lat`. \n",
    "\n",
    "Import the following python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import os\n",
    "from sklearn.cluster import KMeans\n",
    "from yellowbrick.cluster import KElbowVisualizer\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART I: Handling time series with pandas (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **8/10**\n",
    "\n",
    "Merge the `CO2_sensor_measurements.csv`, `temperature_humidity.csv`, and `sensors_metadata.csv`, into a single dataframe. \n",
    "\n",
    "* The merged dataframe contains:\n",
    "    - index: the time instance `timestamp` of the measurements\n",
    "    - columns: the location of the site `LocationName`, the sensor ID `SensorUnit_ID`, the CO2 measurement `CO2`, the `temperature`, the `humidity`, the `zone`, the `altitude`, the longitude `lon` and the latitude `lat`.\n",
    "\n",
    "| timestamp | LocationName | SensorUnit_ID | CO2 | temperature | humidity | zone | altitude | lon | lat |\n",
    "|:---------:|:------------:|:-------------:|:---:|:-----------:|:--------:|:----:|:--------:|:---:|:---:|\n",
    "|    ...    |      ...     |      ...      | ... |     ...     |    ...   |  ... |    ...   | ... | ... |\n",
    "\n",
    "\n",
    "\n",
    "* For each measurement (CO2, humidity, temperature), __take the average over an interval of 30 min__. \n",
    "\n",
    "* If there are missing measurements, __interpolate them linearly__ from measurements that are close by in time.\n",
    "\n",
    "__Hints__: The following methods could be useful\n",
    "\n",
    "1. ```python \n",
    "pandas.DataFrame.resample()\n",
    "``` \n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.resample.html\n",
    "    \n",
    "2. ```python\n",
    "pandas.DataFrame.interpolate()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.interpolate.html\n",
    "    \n",
    "3. ```python\n",
    "pandas.DataFrame.mean()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.mean.html\n",
    "    \n",
    "4. ```python\n",
    "pandas.DataFrame.append()\n",
    "```\n",
    "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.append.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_measurements = pd.read_csv(\"../data/carbosense-raw/CO2_sensor_measurements.csv\",\n",
    "                                sep=\"\\t\",\n",
    "                                parse_dates=['timestamp'])\n",
    "# rename for consistency of column names accross the dataframes\n",
    "co2_measurements = co2_measurements.rename({'SensorUnit_ID':'sensor'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_measurements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_measurements.isnull().any(None) # verify null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_measurements['sensor'] = co2_measurements['sensor'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to resample the C02 measurements dataframe\n",
    "def co2_measurements_resample(df):\n",
    "    res = df['CO2'].resample('30min').mean().to_frame()\n",
    "    # Some periods of 30min have 0 datapoints, therefore we need to interpolate\n",
    "    res = res.interpolate('linear', axis=0)\n",
    "    # save the location name for each sensor\n",
    "    res['LocationName'] = df['LocationName'].values[0]\n",
    "    return res\n",
    "\n",
    "co2_measurements = co2_measurements.set_index('timestamp') \\\n",
    "                                   .groupby('sensor') \\\n",
    "                                   .apply(co2_measurements_resample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "co2_measurements.isnull().any(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity = pd.read_csv(\"../data/carbosense-raw/temperature_humidity.csv\",\n",
    "                            sep=\"\\t\",\n",
    "                            parse_dates=['Timestamp'])\n",
    "# rename for consistency of column names accross the dataframes\n",
    "temp_humidity = temp_humidity.rename({'Timestamp':'timestamp'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mely the dataframe into long format keeping only the timestamp\n",
    "temp_humidity = pd.melt(temp_humidity, id_vars='timestamp', var_name='sensor.temp_humidity', value_name='measurement')\n",
    "# split the Var column to get the sensor ID and the column name in seperate columns\n",
    "temp_humidity[['sensor','temp_humidity']] = temp_humidity['sensor.temp_humidity'].str.split('.', expand=True)\n",
    "# Finally pivot the dataframe to get it in a desired format\n",
    "temp_humidity = temp_humidity.pivot(index=['timestamp','sensor'], columns='temp_humidity', values='measurement').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity['sensor'] = temp_humidity['sensor'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity.isnull().any(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the temperature humidity dataframe every 30 min\n",
    "temp_humidity = temp_humidity.set_index('timestamp') \\\n",
    "                             .groupby('sensor') \\\n",
    "                             .apply(lambda df: df[['temperature', 'humidity']]\n",
    "                                               .interpolate('linear', axis=0) \\\n",
    "                                               .resample('30min').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity.isnull().any(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(\"../data/carbosense-raw/sensors_metadata.csv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all three dataframes together\n",
    "temp_humidity = temp_humidity.reset_index()\n",
    "co2_measurements = co2_measurements.reset_index()\n",
    "final_df = pd.merge(temp_humidity, co2_measurements, how='inner', right_on=['sensor','timestamp'], left_on=['sensor','timestamp'])\n",
    "final_df = pd.merge(final_df, metadata, left_on='LocationName', right_on='LocationName', validate='m:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.isnull().any(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = final_df.set_index('timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **2/10** \n",
    "\n",
    "Export the curated and ready to use timeseries to a csv file, and properly push the merged csv to Git LFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs track -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = 'data/carbosense-raw/final_df.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs track data/carbosense-raw/final_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git lfs track -l "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(save_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git add  data/carbosense-raw/final_df.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git commit -m \"df csv file\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('notebooks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART II: Data visualization (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **5/15** \n",
    "Group the sites based on their altitude, by performing K-means clustering. \n",
    "- Find the optimal number of clusters using the [Elbow method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)). \n",
    "- Wite out the formula of metric you use for Elbow curve. \n",
    "- Perform clustering with the optimal number of clusters and add an additional column `altitude_cluster` to the dataframe of the previous question indicating the altitude cluster index. \n",
    "- Report your findings.\n",
    "\n",
    "__Note__: [Yellowbrick](http://www.scikit-yb.org/) is a very nice Machine Learning Visualization extension to scikit-learn, which might be useful to you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data= metadata[['LocationName', 'altitude', 'lat', 'lon']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting altitude values of the sites \n",
    "X= site_data.altitude.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we use the Elbow method to choose the optimal number of clusters\n",
    "# We use KElbowVisualizer from Yelloybrick library to fit the model with different K values and choose the most optimal\n",
    "model= KMeans()\n",
    "visualizer = KElbowVisualizer(model, k=(2,12), timings = False)\n",
    "visualizer.fit(X)\n",
    "visualizer.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the visualization in the previous cell, the optimal k by the elbow method is 4\n",
    "optimal_k = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fit the model with the optimal number of clusters\n",
    "model = KMeans(n_clusters= optimal_k)\n",
    "model.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add the assigned cluster index to the dataframe\n",
    "assigned_clusters = model.labels_\n",
    "site_data['altitude_cluster'] = assigned_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We add the altitude_cluster column to the dataframe \n",
    "final_df = pd.merge(final_df.reset_index(), site_data[[\"LocationName\", \"altitude_cluster\"]], on= \"LocationName\").set_index(\"timestamp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **4/15** \n",
    "\n",
    "Use `plotly` (or other similar graphing libraries) to create an interactive plot of the monthly median CO2 measurement for each site with respect to the altitude. \n",
    "\n",
    "Add proper title and necessary hover information to each point, and give the same color to stations that belong to the same altitude cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = co2_measurements.groupby(by='LocationName').median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns= [\"sensor\"]).rename(columns= {\"CO2\" : \"CO2_median\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the two dataframes found\n",
    "result = pd.merge(df, site_data, on= 'LocationName') \n",
    "result[\"altitude_cluster\"] = result[\"altitude_cluster\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we visualize the monthly median as a function of the altitude colored according to the clustering result\n",
    "px.scatter(result, x= 'altitude', y= 'CO2_median', color= 'altitude_cluster', title= \"Clustering of sensors by altitude\", hover_name=\"LocationName\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) **6/15**\n",
    "\n",
    "Use `plotly` (or other similar graphing libraries) to plot an interactive time-varying density heatmap of the mean daily CO2 concentration for all the stations. Add proper title and necessary hover information.\n",
    "\n",
    "__Hints:__ Check following pages for more instructions:\n",
    "- [Animations](https://plotly.com/python/animations/)\n",
    "- [Density Heatmaps](https://plotly.com/python/mapbox-density-heatmaps/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We compute the mean daily CO2 measurements for each site \n",
    "daily_co2_measurements= co2_measurements.groupby(by = [\"LocationName\", co2_measurements['timestamp'].dt.day]).mean().drop(columns= [\"sensor\"]).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_co2_measurements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We merge the two dataframes \n",
    "daily_co2_measurements = pd.merge(daily_co2_measurements, site_data, on= [\"LocationName\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_co2_measurements.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_co2_measurements = daily_co2_measurements.rename(columns={'timestamp' : 'day'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute min and max CO2 mean \n",
    "min_CO2_mean= s.CO2.values.min()\n",
    "max_CO2_mean = s.CO2.values.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.density_mapbox(daily_co2_measurements, lat='lat', lon='lon', z='CO2', radius=10,\n",
    "                        center=dict(lat=daily_co2_measurements.lat.mean(), lon=daily_co2_measurements.lon.mean()), zoom=11,\n",
    "                        mapbox_style=\"stamen-terrain\", animation_frame='day', animation_group= 'LocationName', \n",
    "                        hover_name='LocationName', title= 'Time varying Density heatmap of mean CO2 measurements per day',\n",
    "                        range_color=[0, 500], height=900, width=900, opacity=0.8)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PART III: Model fitting for data curation (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) **2/35**\n",
    "\n",
    "The domain experts in charge of these sensors report that one of the CO2 sensors `ZSBN` is exhibiting a drift on Oct. 24. Verify the drift by visualizing the CO2 concentration of the drifting sensor and compare it with some other sensors from the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only C02 values after Oct 20 and for a few selected regions in order to compare\n",
    "df_after_oct_20 = final_df[final_df.index.day >= 20]\n",
    "df_comparison = df_after_oct_20[df_after_oct_20['LocationName'].isin(['ZSBN','ZLDW','SMHK','ZWCH'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.line(df_comparison,\n",
    "              y='CO2',\n",
    "              color='LocationName',\n",
    "              labels={\n",
    "                'timestamp':'time',\n",
    "                'CO2':'CO2 (ppm)',\n",
    "                'LocationName':'Sensor'\n",
    "              }, \n",
    "              title='CO2 Level (ppm) after October 20th'\n",
    "             )\n",
    "fig.update_xaxes(\n",
    "    dtick=24*60*60*1000\n",
    ")\n",
    "fig.update_layout(\n",
    "    hovermode='x unified'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) **8/35**\n",
    "\n",
    "The domain experts ask you if you could reconstruct the CO2 concentration of the drifting sensor had the drift not happened. You decide to:\n",
    "- Fit a linear regression model to the CO2 measurements of the site, by considering as features the covariates not affected by the malfunction (such as temperature and humidity)\n",
    "- Create an interactive plot with `plotly` (or other similar graphing libraries):\n",
    "    - the actual CO2 measurements\n",
    "    - the values obtained by the prediction of the linear model for the entire month of October\n",
    "    - the __confidence interval__ obtained from cross validation\n",
    "- What do you observe? Report your findings.\n",
    "\n",
    "__Note:__ Cross validation on time series is different from that on other kinds of datasets. The following diagram illustrates the series of training sets (in orange) and validation sets (in blue). For more on time series cross validation, there are a lot of interesting articles available online. scikit-learn provides a nice method [`sklearn.model_selection.TimeSeriesSplit`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html).\n",
    "\n",
    "![ts_cv](https://player.slideplayer.com/86/14062041/slides/slide_28.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import scipy.stats\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataframe for regression, only ZSBN data is kept, with the unaffected covariates\n",
    "reg_df = final_df.loc[final_df['LocationName'] == 'ZSBN', ['temperature', 'humidity', 'CO2']]\n",
    "reg_df['time'] = reg_df.reset_index().index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that performs rolling cross validation and returns the best model (According to MSE)\n",
    "# and the used training set\n",
    "def rolling_cross_validation(X, y):\n",
    "    # split the dataset\n",
    "    series_split = TimeSeriesSplit(n_splits=50)\n",
    "    model = LinearRegression()\n",
    "    min_mse = np.inf\n",
    "    final_train_ind = None\n",
    "    # iterate on the different train-test split in order to pick the best model\n",
    "    for train_indices, test_indices in series_split.split(X):\n",
    "        train_x, train_y = X[train_indices], y[train_indices]\n",
    "        test_x, test_y = X[test_indices], y[test_indices]\n",
    "        model.fit(train_x, train_y)\n",
    "        mse = mean_squared_error(test_y, model.predict(test_x))\n",
    "        if mse <= min_mse:\n",
    "            min_mse = mse\n",
    "            final_train_ind = train_indices\n",
    "    return final_train_ind "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep values before OCT 24 for training\n",
    "X_train = reg_df.loc[reg_df.index.day < 24 , ['temperature', 'humidity', 'time']].values\n",
    "y_train = reg_df.loc[reg_df.index.day < 24, 'CO2'].values\n",
    "X_pred = reg_df[['temperature', 'humidity', 'time']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = rolling_cross_validation(X_train, y_train)\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use StatsModels in order to compute the 95% CIs\n",
    "ols = sm.OLS(y_train, sm.add_constant(X_train))\n",
    "results = ols.fit()\n",
    "predictions = results.get_prediction(sm.add_constant(X_pred))\n",
    "conf_intervals =  predictions.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_df['lower_ci'] = conf_intervals[:, 0]\n",
    "reg_df['upper_ci'] = conf_intervals[:, 1]\n",
    "reg_df['predicted'] = predictions.predicted_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure([go.Scatter(x=reg_df.index,\n",
    "                              y=reg_df['predicted'],\n",
    "                              mode='lines', name='Predicted'), \n",
    "                 go.Scatter(x=reg_df.index,\n",
    "                            y=reg_df['CO2'],\n",
    "                            mode='lines', name='Measured'),\n",
    "                 go.Scatter( x=reg_df.index.append(reg_df.index[::-1]),\n",
    "                             y=list(reg_df['upper_ci']) + list(reg_df['lower_ci'][::-1]),\n",
    "                             fill='toself',\n",
    "                             fillcolor='rgba(0,100,80,0.2)',\n",
    "                             line=dict(color='rgba(255,255,255,0)'),\n",
    "                             name='95 % CI')\n",
    "                 ])\n",
    "\n",
    "fig.update_xaxes(dtick=24*60*60*1000)\n",
    "fig.update_layout(hovermode='x unified', title='Measured vs Predicted CO2 levels For ZSBN Sensor in October')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) **10/35**\n",
    "\n",
    "In your next attempt to solve the problem, you decide to exploit the fact that the CO2 concentrations, as measured by the sensors __experiencing similar conditions__, are expected to be similar.\n",
    "\n",
    "- Find the sensors sharing similar conditions with `ZSBN`. Explain your definition of \"similar condition\".\n",
    "- Fit a linear regression model to the CO2 measurements of the site, by considering as features:\n",
    "    - the information of provided by similar sensors\n",
    "    - the covariates associated with the faulty sensors that were not affected by the malfunction (such as temperature and humidity).\n",
    "- Create an interactive plot with `plotly` (or other similar graphing libraries):\n",
    "    - the actual CO2 measurements\n",
    "    - the values obtained by the prediction of the linear model for the entire month of October\n",
    "    - the __confidence interval__ obtained from cross validation\n",
    "- What do you observe? Report your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answer\n",
    "\n",
    "We can consider \"similar conditions\" as being in close altitude and in close environmental status on average. Hence similar sensors as ZSBN are sensors that are in the same cluster and that have the same temperature and humidity on average as ZSBN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver_conditions= final_df.groupby(\"LocationName\").mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver_conditions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#aver_conditions = pd.merge(aver_conditions.reset_index(), result, on= \"LocationName\")[[\"LocationName\", \"temperature\", \"humidity\", \"altitude_cluster\"]]\n",
    "aver_conditions = aver_conditions.reset_index()[[\"LocationName\", \"temperature\", \"humidity\", \"altitude_cluster\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver_conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the cluster of ZSBN sensor\n",
    "zsbn_cluster = aver_conditions[aver_conditions.LocationName == \"ZSBN\"].altitude_cluster.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking only sensors in the same cluster as ZSBN\n",
    "aver_conditions = aver_conditions[aver_conditions.altitude_cluster == zsbn_cluster]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aver_conditions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting average humidity and average temperature of ZSBN sensor\n",
    "zsbn_humidity = aver_conditions[aver_conditions.LocationName == \"ZSBN\"][\"humidity\"].iloc[0]\n",
    "zsbn_temp = aver_conditions[aver_conditions.LocationName == \"ZSBN\"][\"temperature\"].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting similar sensors. Here the choice of the constants can be changed\n",
    "similar = aver_conditions.loc[(np.abs(aver_conditions.temperature - zsbn_temp) <= 0.5) & (np.abs(aver_conditions.humidity - zsbn_humidity) <= 1) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting the location name of the sensors as a set\n",
    "similar_sensors = set(similar[similar[\"LocationName\"] != \"ZSBN\"].LocationName.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_sensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = final_df[final_df.LocationName == \"ZSBN\"][[\"temperature\", \"humidity\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the feature vector i.e [ZSBN_temperature, ZSBN_humidity, CO2_measurements_of_similar_sensors]\n",
    "for sensor in similar_sensors:\n",
    "    t = final_df[final_df.LocationName == sensor][\"CO2\"]\n",
    "    t = np.array(t).reshape(-1,1)\n",
    "    X = np.concatenate((X, t), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing the target vector : CO2 measurement of the faulty sensor ZSBN\n",
    "Y= np.array(final_df[final_df.LocationName == \"ZSBN\"][\"CO2\"]).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit linear regression model\n",
    "model= LinearRegression()\n",
    "model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"actual_CO2\"]= final_df[final_df.LocationName == \"ZSBN\"][\"CO2\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features[\"predicted_CO2\"]= model.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d) **10/35**\n",
    "\n",
    "Now, instead of feeding the model with all features, you want to do something smarter by using linear regression with fewer features.\n",
    "\n",
    "- Start with the same sensors and features as in question c)\n",
    "- Leverage at least two different feature selection methods\n",
    "- Create similar interactive plot as in question c)\n",
    "- Describe the methods you choose and report your findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Feature selection method: Univariate selection based on F-score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to select the most relevant features is by using Univariate selection. The method consists of computing the correlation between each feature and the target variable (Here, the CO2 measurement). We then keep the most k correlated features.\n",
    "This approach detects only linear relationship between the covariate and the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import f_regression\n",
    "from sklearn.feature_selection import SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(f_regression, k=2).fit_transform(X, Y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= LinearRegression()\n",
    "model.fit(X_new, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e) **5/35**\n",
    "\n",
    "Eventually, you'd like to try something new - __Bayesian Structural Time Series Modelling__ - to reconstruct counterfactual values, that is, what the CO2 measurements of the faulty sensor should have been, had the malfunction not happened on October 24. You will use:\n",
    "- the information of provided by similar sensors - the ones you identified in question c)\n",
    "- the covariates associated with the faulty sensors that were not affected by the malfunction (such as temperature and humidity).\n",
    "\n",
    "To answer this question, you can choose between a Python port of the CausalImpact package (such as https://github.com/dafiti/causalimpact) or the original R version (https://google.github.io/CausalImpact/CausalImpact.html) that you can run in your notebook via an R kernel (https://github.com/IRkernel/IRkernel).\n",
    "\n",
    "Before you start, watch first the [presentation](https://www.youtube.com/watch?v=GTgZfCltMm8) given by Kay Brodersen (one of the creators of the causal impact implementation in R), and this introductory [ipython notebook](http://nbviewer.jupyter.org/github/dafiti/causalimpact/blob/master/examples/getting_started.ipynb) with examples of how to use the python package.\n",
    "\n",
    "- Report your findings:\n",
    "    - Is the counterfactual reconstruction of CO2 measurements significantly different from the observed measurements?\n",
    "    - Can you try to explain the results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causalimpact import CausalImpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = final_df[final_df.LocationName == 'ZSBN'][['CO2','temperature','humidity']]\n",
    "d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similar_df = final_df[final_df.LocationName.isin(similar_sensors)][['CO2','LocationName','temperature','humidity']]\n",
    "similar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = list(d.columns)\n",
    "[cols.append(x) for x in similar_df.LocationName.unique()]\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = d.join(similar_df.pivot(columns='LocationName', values='CO2'))[cols]\n",
    "observations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci = CausalImpact(observations,\n",
    "                  ['2017-10-01 00:00:00', '2017-10-23 23:30:00'],\n",
    "                  ['2017-10-24 00:00:00', '2017-10-31 23:30:00'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ci.plot(figsize=(20, 15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ci.summary(output='report'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's all, folks!"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
